{
    "abstract": "In this paper, we study flat and hierarchical classification strategies in the context of large-scale taxonomies. Addressing the problem from a learning-theoretic point of view, we first propose a multi-class, hierarchical data dependent bound on the generalization error of classifiers deployed in large-scale taxonomies. This bound provides an explanation to several empirical results reported in the literature, related to the performance of flat and hierarchical classifiers. Based on this bound, we also propose a technique for modifying a given taxonomy through pruning, that leads to a lower value of the upper bound as compared to the original taxonomy. We then present another method for hierarchy pruning by studying approximation error of a family of classifiers, and derive from it features used in a meta-classifier to decide which nodes to prune. We finally illustrate the theoretical developments through several experiments conducted on two widely used taxonomies.",
    "authors": [
        "Rohit Babbar",
        "Ioannis Partalas",
        "Eric Gaussier",
        "Massih-Reza Amini",
        "C\\'{e}cile Amblard"
    ],
    "id": "14-207",
    "issue": 98,
    "pages": [
        1,
        37
    ],
    "title": "Learning Taxonomy Adaptation in Large-scale Classification",
    "volume": 17,
    "year": 2016
}